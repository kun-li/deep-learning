{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ce7c01",
   "metadata": {},
   "source": [
    "<style>\n",
    "    \n",
    "    .rendered_html p {\n",
    "        font-size: 16px;\n",
    "    }\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9368d",
   "metadata": {},
   "source": [
    "This data set is from Kaggle's competition \"Natural Language Processing with Disaster Tweets\" from https://www.kaggle.com/competitions/nlp-getting-started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "154befd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "50976d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kunli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b98c7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f992e84",
   "metadata": {},
   "source": [
    "## Read data and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be81f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a74fb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                                                                                 text  target\n",
       "0   1     NaN      NaN                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1\n",
       "1   4     NaN      NaN                                                               Forest fire near La Ronge Sask. Canada       1\n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...       1\n",
       "3   6     NaN      NaN                                    13,000 people receive #wildfires evacuation orders in California        1\n",
       "4   7     NaN      NaN             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school        1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9d303dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4342</td>\n",
       "      <td>4323</td>\n",
       "      <td>2884</td>\n",
       "      <td>4342</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3271</td>\n",
       "      <td>3229</td>\n",
       "      <td>2196</td>\n",
       "      <td>3271</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  keyword  location  text  hashtag\n",
       "target                                        \n",
       "0       4342     4323      2884  4342      885\n",
       "1       3271     3229      2196  3271      858"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check disaster and non-disaster counts \n",
    "\n",
    "df.groupby('target').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc15aca",
   "metadata": {},
   "source": [
    "#### I think hashtag # is important, espeically the texts after #, so I extracted the words after # in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d35233bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtag'] = df['text'].str.extractall(r'#(\\w+)').groupby(level=0).agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd1e507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate positive and negative tweets to analyze\n",
    "\n",
    "positive_tweet = df[df['target'] == 1]\n",
    "negative_tweet = df[df['target'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffb1c4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    3457\n",
       "1    2413\n",
       "Name: hashtag, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_by_group = df.groupby('target')['hashtag'].apply(lambda x: x.isnull().sum())\n",
    "missing_values_by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b90f1e",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d06b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean symbols. reference: https://python.plainenglish.io/nlp-twitter-sentiment-analysis-using-python-ml-4b4a8fc1e2b \n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = contractions.fix(txt)\n",
    "    txt = re.sub(r\"RT[\\s]+\", \"\", txt)\n",
    "    txt = txt.replace(\"\\n\", \" \")\n",
    "    txt = re.sub(\" +\", \" \", txt)\n",
    "    txt = re.sub(r\"https?:\\/\\/\\S+\", \"\", txt)\n",
    "    txt = re.sub(r\"(@[A-Za-z0–9_]+)|[^\\w\\s]|#\", \"\", txt)\n",
    "    #txt = emoji.replace_emoji(txt, replace='')\n",
    "    tokens = txt.split()\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    txt.join(filtered)\n",
    "    txt.strip()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4c92c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "581fbd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>our deeds are the reason of this earthquake may allah forgive us all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are being notified by officers no other evacuation or sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>wildfires</td>\n",
       "      <td>13000 people receive wildfires evacuation orders in california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>Alaska wildfires</td>\n",
       "      <td>just got sent this photo from ruby alaska as smoke from wildfires pours into a school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding a bridge collapse into nearby homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the out of control wild fires in california even in the northern part of the state very troubling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m194 0104 utc5km s of volcano hawaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffer...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after an ebike collided with a car in little portugal ebike rider suffered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the latest more homes razed by northern california wildfire  abc news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location                                                                                                 text  target           hashtag                                                                                                clean\n",
       "0         1     NaN      NaN                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1        earthquake                                 our deeds are the reason of this earthquake may allah forgive us all\n",
       "1         4     NaN      NaN                                                               Forest fire near La Ronge Sask. Canada       1               NaN                                                                forest fire near la ronge sask canada\n",
       "2         5     NaN      NaN  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...       1               NaN  all residents asked to shelter in place are being notified by officers no other evacuation or sh...\n",
       "3         6     NaN      NaN                                    13,000 people receive #wildfires evacuation orders in California        1         wildfires                                      13000 people receive wildfires evacuation orders in california \n",
       "4         7     NaN      NaN             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school        1  Alaska wildfires               just got sent this photo from ruby alaska as smoke from wildfires pours into a school \n",
       "...     ...     ...      ...                                                                                                  ...     ...               ...                                                                                                  ...\n",
       "7608  10869     NaN      NaN                  Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5       1               NaN                                        two giant cranes holding a bridge collapse into nearby homes \n",
       "7609  10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part ...       1               NaN    the out of control wild fires in california even in the northern part of the state very troubling\n",
       "7610  10871     NaN      NaN                                    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ       1               NaN                                                                m194 0104 utc5km s of volcano hawaii \n",
       "7611  10872     NaN      NaN  Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffer...       1               NaN  police investigating after an ebike collided with a car in little portugal ebike rider suffered ...\n",
       "7612  10873     NaN      NaN       The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d       1               NaN                               the latest more homes razed by northern california wildfire  abc news \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords, stemming\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f761a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column where numbers are replaced with the word 'quantity'. Because disater tweet always have numbers of people killed, items destroyed. \n",
    "\n",
    "def replace_numbers_with_quantity(text):\n",
    "    # Use regular expression to match numbers\n",
    "    pattern = r'\\b\\d+\\b'\n",
    "    replaced_text = re.sub(pattern, 'quantity', text)\n",
    "    return replaced_text\n",
    "\n",
    "# Test the function\n",
    "df['no_number'] = df['clean'].apply(replace_numbers_with_quantity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46051d",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5114e01",
   "metadata": {},
   "source": [
    "Reference https://www.kaggle.com/code/artemzapara/twitter-feeds-classification-with-glove-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d0dcc507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove data loaded! In total: 1193514  words.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.074481 ,  0.46586  , -0.21778  ,  0.0045365, -0.95808  ,\n",
       "       -0.35728  , -0.47167  , -0.55515  , -0.084384 , -0.32704  ,\n",
       "       -0.040345 , -0.77329  , -0.51059  ,  0.26916  , -0.1164   ,\n",
       "       -0.25205  ,  0.5357   , -0.04781  ,  0.044159 ,  0.67384  ,\n",
       "       -0.60659  , -0.19984  , -0.052206 , -0.080398 , -0.47106  ,\n",
       "        0.97058  , -0.23209  , -0.43689  ,  0.22923  ,  0.70862  ,\n",
       "       -0.26816  ,  0.055069 ,  0.088242 ,  0.11365  , -0.3306   ,\n",
       "       -0.49205  ,  0.090598 ,  0.026713 , -0.2069   ,  0.32853  ,\n",
       "        0.4126   ,  0.052834 , -1.0666   , -0.076318 , -0.42735  ,\n",
       "        0.51737  ,  0.2213   ,  0.52193  , -0.48834  , -0.45973  ,\n",
       "        0.55082  ,  0.21589  , -0.0018647, -0.065373 , -0.93301  ,\n",
       "        0.39697  , -0.44072  ,  0.90262  ,  0.010177 ,  0.60577  ,\n",
       "       -0.28581  ,  1.0762   , -0.52111  ,  0.099238 , -0.015993 ,\n",
       "        0.050768 ,  0.32227  ,  0.0712   , -0.45601  , -0.40253  ,\n",
       "       -0.40376  ,  0.16402  ,  0.69729  , -0.15296  ,  0.026494 ,\n",
       "       -0.030855 , -0.4009   ,  0.33016  ,  0.0072752, -0.41506  ,\n",
       "        0.85041  ,  0.67514  , -0.027111 ,  0.43421  , -0.11408  ,\n",
       "       -0.081548 ,  0.74204  ,  0.65664  , -0.06726  , -0.030495 ,\n",
       "       -0.040529 , -0.052097 ,  0.1399   , -0.6867   , -0.39945  ,\n",
       "       -0.29605  , -0.14358  ,  0.27786  , -0.42035  ,  0.32748  ,\n",
       "        0.40251  ,  1.4535   ,  0.40689  ,  0.16354  ,  0.93783  ,\n",
       "       -0.057914 , -0.17441  ,  0.68029  ,  0.84772  , -0.20781  ,\n",
       "       -0.4702   , -0.078686 ,  0.54689  , -0.54321  ,  0.10812  ,\n",
       "       -0.2428   ,  0.19962  ,  0.3812   , -0.16949  ,  0.6044   ,\n",
       "        0.42907  , -0.11161  ,  0.22621  ,  0.26738  , -1.1739   ,\n",
       "        0.34345  , -0.52993  , -0.39223  ,  0.7318   , -0.47859  ,\n",
       "        0.20976  , -0.82578  , -0.62387  , -0.19727  ,  0.6326   ,\n",
       "       -1.0065   , -0.51335  ,  0.76642  ,  0.24233  , -0.49883  ,\n",
       "       -0.28816  , -0.060823 , -0.21854  ,  0.22359  ,  0.3498   ,\n",
       "       -0.46434  , -0.39527  , -0.032111 , -0.49294  ,  0.57203  ,\n",
       "       -0.48808  ,  0.71283  , -0.79679  , -1.1305   , -0.026092 ,\n",
       "        0.10953  ,  0.66619  , -0.70197  ,  0.39926  , -0.13387  ,\n",
       "       -0.011027 , -0.51316  ,  0.046891 ,  0.24789  ,  0.031932 ,\n",
       "       -0.35607  , -0.85644  , -0.37695  ,  0.54626  , -0.045634 ,\n",
       "       -0.56854  ,  0.018243 , -0.011948 , -0.43889  ,  0.17385  ,\n",
       "       -0.30815  ,  0.2058   , -0.037131 ,  0.30172  , -0.13348  ,\n",
       "        0.14206  , -0.094229 , -0.37307  , -0.09812  ,  0.21421  ,\n",
       "        0.35927  ,  1.1372   ,  0.065806 ,  0.30408  ,  0.018483 ,\n",
       "        0.1911   ,  0.46481  ,  0.13349  ,  0.43825  ,  0.49833  ,\n",
       "        0.068101 ,  0.22804  , -0.18258  , -0.35925  ,  0.30587  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word embeddings from GloVe's twitter vector https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "path_to_glove_file = 'glove.twitter.27B.200d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(path_to_glove_file, 'r', encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    splitLine = line.split(' ')\n",
    "    word = splitLine[0]                                  # the first entry is the word\n",
    "    coefs = np.asarray(splitLine[1:], dtype='float32')   # these are the vectors representing word embeddings\n",
    "    embeddings_index[word] = coefs\n",
    "print(\"Glove data loaded! In total:\",len(embeddings_index),\" words.\")\n",
    "\n",
    "embeddings_index['wildfire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ede92a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/code/artemzapara/twitter-feeds-classification-with-glove-embeddings\n",
    "\n",
    "def train_val_split(df, text_col, target_col, validation_split):\n",
    "    \"\"\"\n",
    "    This function generates the training and validation splits from an input dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe: pandas dataframe with columns \"text\" and \"target\" (binary)\n",
    "        validation_split: should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the validation split\n",
    "        text_col, target_col: column names for the text that needs classified and the target as the labels\n",
    "    \n",
    "    Returns:\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        train_labels: list of labels (0 or 1) in the training dataset\n",
    "        val_labels: list of labels (0 or 1) in the validation dataset      \n",
    "    \"\"\"\n",
    "       \n",
    "    text = df[text_col].values.tolist()                         # input text as list\n",
    "    targets = df[target_col].values.tolist()                    # targets\n",
    "    \n",
    "#   Preparing the training/validation datasets\n",
    "    \n",
    "    seed = random.randint(1,50)   # random integer in a range (1, 50)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(text)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(targets)\n",
    "\n",
    "    num_validation_samples = int(validation_split * len(text))\n",
    "\n",
    "    train_samples = text[:-num_validation_samples]\n",
    "    val_samples = text[-num_validation_samples:]\n",
    "    train_labels = targets[:-num_validation_samples]\n",
    "    val_labels = targets[-num_validation_samples:]\n",
    "    \n",
    "    print(f\"Total size of the dataset: {df.shape[0]}.\")\n",
    "    print(f\"Training dataset: {len(train_samples)}.\")\n",
    "    print(f\"Validation dataset: {len(val_samples)}.\")\n",
    "    \n",
    "    return train_samples, val_samples, train_labels, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "49d2f029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the dataset: 7613.\n",
      "Training dataset: 6472.\n",
      "Validation dataset: 1141.\n"
     ]
    }
   ],
   "source": [
    "train_samples, val_samples, train_labels, val_labels = train_val_split(df, 'no_number', 'target', 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3ddcf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(train_samples, val_samples, embeddings_index):\n",
    "    \"\"\"\n",
    "    This function computes the embedding matrix that will be used in the embedding layer\n",
    "    \n",
    "    Parameters:\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        embeddings_index: Python dictionary with word embeddings\n",
    "    \n",
    "    Returns:\n",
    "        embedding_matrix: embedding matrix with the dimensions (num_tokens, embedding_dim), where num_tokens is the vocabulary of the input data, and emdebbing_dim is the number of components in the GloVe vectors (can be 50,100,200,300)\n",
    "        vectorizer: TextVectorization layer      \n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = TextVectorization(max_tokens=30000, output_sequence_length=50)\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "    vectorizer.adapt(text_ds)\n",
    "    \n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "      \n",
    "    num_tokens = len(voc)\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "#   creating an embedding matrix\n",
    "    embedding_dim = len(embeddings_index['the'])\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "#     print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    print(f\"Converted {hits} words ({misses} misses).\")\n",
    "\n",
    "    return embedding_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fb27150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 11277 words (2797 misses).\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "62e1ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nn(embedding_matrix):\n",
    "    \"\"\"\n",
    "    This function initializes Keras model for binary text classification\n",
    "    \n",
    "    Parameters:\n",
    "        embedding matrix with the dimensions (num_tokens, embedding_dim), where num_tokens is the vocabulary size of the input data, and emdebbing_dim is the number of components in the GloVe vectors\n",
    "    \n",
    "    Returns:\n",
    "        model: Keras model    \n",
    "    \"\"\"\n",
    "    \n",
    "    num_tokens = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False,                # we are not going to train the embedding vectors\n",
    "    )\n",
    "    \n",
    "#   Here we define the architecture of the Keras model. \n",
    "    int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = embedding_layer(int_sequences_input) \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.LSTM(128,return_sequences=True)(x)\n",
    "    x = layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5fbd6d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 200)         2814800   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 200)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 128)         168448    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         49280     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3040849 (11.60 MB)\n",
      "Trainable params: 226049 (883.00 KB)\n",
      "Non-trainable params: 2814800 (10.74 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initial_model = initialize_nn(embedding_matrix)\n",
    "initial_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9159ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_samples, val_samples, train_labels, val_labels, vectorizer, stop = False):\n",
    "    \"\"\"\n",
    "    This function fits the training data using validation data to calculate metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        model: preinitialized Keras model\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        train_labels: list of labels (0 or 1) in the training dataset\n",
    "        val_labels: list of labels (0 or 1) in the validation dataset\n",
    "        vectorizer: TextVectorization layer\n",
    "        stop (Boolean): flag for Early Stopping (aborting training when a monitored metric has stopped improving)\n",
    "    \n",
    "    Returns:\n",
    "        model: trained Keras model\n",
    "        history: callback that can be used to track the learning process\n",
    "    \"\"\"\n",
    "    \n",
    "    print('')\n",
    "    print(\"Training the model...\")\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=\"adam\", \n",
    "              metrics=[\"acc\"])\n",
    "    \n",
    "    x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "    x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "    \n",
    "    y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "    y_val = np.asarray(val_labels).astype('float32').reshape((-1,1))\n",
    "    \n",
    "    if stop:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "        history = model.fit(x_train, y_train, batch_size=32, epochs=40, validation_data=(x_val, y_val), callbacks=[early_stopping], verbose=1)\n",
    "    else:\n",
    "        history = model.fit(x_train, y_train, batch_size=32, epochs=40, validation_data=(x_val, y_val), verbose=1)\n",
    "        \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "83b4bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Epoch 1/40\n",
      "203/203 [==============================] - 17s 70ms/step - loss: 0.2275 - acc: 0.9044 - val_loss: 0.6397 - val_acc: 0.7932\n",
      "Epoch 2/40\n",
      "203/203 [==============================] - 14s 66ms/step - loss: 0.2098 - acc: 0.9169 - val_loss: 0.6280 - val_acc: 0.7651\n",
      "Epoch 3/40\n",
      "203/203 [==============================] - 14s 67ms/step - loss: 0.2118 - acc: 0.9136 - val_loss: 0.6224 - val_acc: 0.7800\n",
      "Epoch 4/40\n",
      "203/203 [==============================] - 14s 67ms/step - loss: 0.1985 - acc: 0.9195 - val_loss: 0.6806 - val_acc: 0.7730\n",
      "Epoch 5/40\n",
      "203/203 [==============================] - 14s 67ms/step - loss: 0.1878 - acc: 0.9240 - val_loss: 0.7816 - val_acc: 0.7730\n",
      "Epoch 6/40\n",
      "203/203 [==============================] - 14s 67ms/step - loss: 0.1876 - acc: 0.9246 - val_loss: 0.6983 - val_acc: 0.7739\n",
      "Epoch 7/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1726 - acc: 0.9260 - val_loss: 0.7193 - val_acc: 0.7774\n",
      "Epoch 8/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1733 - acc: 0.9319 - val_loss: 0.7415 - val_acc: 0.7730\n",
      "Epoch 9/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1648 - acc: 0.9334 - val_loss: 0.7547 - val_acc: 0.7783\n",
      "Epoch 10/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1608 - acc: 0.9342 - val_loss: 0.7756 - val_acc: 0.7677\n",
      "Epoch 11/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1610 - acc: 0.9367 - val_loss: 0.7955 - val_acc: 0.7730\n",
      "Epoch 12/40\n",
      "203/203 [==============================] - 14s 67ms/step - loss: 0.1480 - acc: 0.9397 - val_loss: 0.9015 - val_acc: 0.7677\n",
      "Epoch 13/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1545 - acc: 0.9394 - val_loss: 0.8154 - val_acc: 0.7704\n",
      "Epoch 14/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1472 - acc: 0.9427 - val_loss: 0.8175 - val_acc: 0.7870\n",
      "Epoch 15/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1426 - acc: 0.9425 - val_loss: 0.8538 - val_acc: 0.7660\n",
      "Epoch 16/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1323 - acc: 0.9487 - val_loss: 0.8416 - val_acc: 0.7677\n",
      "Epoch 17/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1406 - acc: 0.9422 - val_loss: 0.8567 - val_acc: 0.7634\n",
      "Epoch 18/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1307 - acc: 0.9493 - val_loss: 0.9135 - val_acc: 0.7520\n",
      "Epoch 19/40\n",
      "203/203 [==============================] - 13s 66ms/step - loss: 0.1326 - acc: 0.9476 - val_loss: 0.9191 - val_acc: 0.7651\n",
      "Epoch 20/40\n",
      "203/203 [==============================] - 14s 69ms/step - loss: 0.1254 - acc: 0.9482 - val_loss: 0.8710 - val_acc: 0.7599\n",
      "Epoch 21/40\n",
      "203/203 [==============================] - 14s 70ms/step - loss: 0.1288 - acc: 0.9470 - val_loss: 0.8765 - val_acc: 0.7520\n",
      "Epoch 22/40\n",
      "203/203 [==============================] - 14s 68ms/step - loss: 0.1294 - acc: 0.9492 - val_loss: 0.8672 - val_acc: 0.7599\n",
      "Epoch 23/40\n",
      "203/203 [==============================] - 14s 68ms/step - loss: 0.1292 - acc: 0.9521 - val_loss: 0.8707 - val_acc: 0.7546\n",
      "Epoch 24/40\n",
      "203/203 [==============================] - 14s 69ms/step - loss: 0.1149 - acc: 0.9527 - val_loss: 0.9741 - val_acc: 0.7528\n",
      "Epoch 25/40\n",
      "203/203 [==============================] - 14s 69ms/step - loss: 0.1165 - acc: 0.9513 - val_loss: 0.9797 - val_acc: 0.7590\n",
      "Epoch 26/40\n",
      "203/203 [==============================] - 14s 67ms/step - loss: 0.1084 - acc: 0.9543 - val_loss: 0.9597 - val_acc: 0.7713\n",
      "Epoch 27/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1125 - acc: 0.9574 - val_loss: 1.0240 - val_acc: 0.7713\n",
      "Epoch 28/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1180 - acc: 0.9509 - val_loss: 0.9635 - val_acc: 0.7607\n",
      "Epoch 29/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1064 - acc: 0.9591 - val_loss: 1.0388 - val_acc: 0.7520\n",
      "Epoch 30/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1094 - acc: 0.9560 - val_loss: 1.0190 - val_acc: 0.7686\n",
      "Epoch 31/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1096 - acc: 0.9558 - val_loss: 1.0567 - val_acc: 0.7458\n",
      "Epoch 32/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1065 - acc: 0.9591 - val_loss: 1.0235 - val_acc: 0.7476\n",
      "Epoch 33/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.0991 - acc: 0.9612 - val_loss: 1.1219 - val_acc: 0.7599\n",
      "Epoch 34/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1022 - acc: 0.9615 - val_loss: 1.0162 - val_acc: 0.7634\n",
      "Epoch 35/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.1032 - acc: 0.9575 - val_loss: 0.9472 - val_acc: 0.7704\n",
      "Epoch 36/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.0972 - acc: 0.9600 - val_loss: 0.9852 - val_acc: 0.7546\n",
      "Epoch 37/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.0966 - acc: 0.9608 - val_loss: 1.0200 - val_acc: 0.7572\n",
      "Epoch 38/40\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.0966 - acc: 0.9628 - val_loss: 1.0495 - val_acc: 0.7739\n",
      "Epoch 39/40\n",
      "203/203 [==============================] - 13s 66ms/step - loss: 0.0972 - acc: 0.9612 - val_loss: 1.0158 - val_acc: 0.7704\n",
      "Epoch 40/40\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.1038 - acc: 0.9618 - val_loss: 1.0115 - val_acc: 0.7739\n"
     ]
    }
   ],
   "source": [
    "model, history = train_nn(initial_model, train_samples, val_samples, train_labels, val_labels, vectorizer, stop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41aba3",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8e0eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_nn(df, model):\n",
    "    \"\"\"\n",
    "    This function generates (binary) targets from a dataframe with column \"text\" using trained Keras model\n",
    "    \n",
    "    Parameters:\n",
    "        df: pandas dataframe with column \"text\"\n",
    "        model: Keras model (trained)\n",
    "    \n",
    "    Output:\n",
    "        predictions: list of suggested targets corresponding to string entries from the column \"text\"\n",
    "    \"\"\"\n",
    "    \n",
    "    string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorizer(string_input)\n",
    "    preds = model(x)\n",
    "    end_to_end_model = keras.Model(string_input, preds)\n",
    "    \n",
    "    probabilities = end_to_end_model.predict(df[\"no_number\"])\n",
    "    \n",
    "    predictions = [1 if i > 0.5 else 0 for i in probabilities]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d003f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8f2e9ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTENERS XrWn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city&amp;amp;3others hardest hit. My yard looks like it wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/UtbXLcBIuY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Emergency Plan. #yycstorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location                                                                                                 text\n",
       "0         0     NaN      NaN                                                                   Just happened a terrible car crash\n",
       "1         2     NaN      NaN                                     Heard about #earthquake is different cities, stay safe everyone.\n",
       "2         3     NaN      NaN     there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\n",
       "3         9     NaN      NaN                                                             Apocalypse lighting. #Spokane #wildfires\n",
       "4        11     NaN      NaN                                                        Typhoon Soudelor kills 28 in China and Taiwan\n",
       "...     ...     ...      ...                                                                                                  ...\n",
       "3258  10861     NaN      NaN                                              EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTENERS XrWn\n",
       "3259  10865     NaN      NaN  Storm in RI worse than last hurricane. My city&amp;3others hardest hit. My yard looks like it wa...\n",
       "3260  10868     NaN      NaN                                              Green Line derailment in Chicago http://t.co/UtbXLcBIuY\n",
       "3261  10874     NaN      NaN                                    MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3\n",
       "3262  10875     NaN      NaN                                 #CityofCalgary has activated its Municipal Emergency Plan. #yycstorm\n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "68c4b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['clean'] = test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e3e81989",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['no_number'] = test['clean'].apply(replace_numbers_with_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bd25f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 3s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = suggest_nn(test, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a8c76e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = {\"Id\": test.id.tolist(), \"target\": predictions}\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac973919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
